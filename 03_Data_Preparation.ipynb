{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "mKO04lfj9ZeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecEQGO9_Kuuu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Synthetic Construction Management Dataset\n",
        "data = {\n",
        "    'Project_ID': [f'P{i:03d}' for i in range(1, 21)],\n",
        "    'Project_Cost_USD': np.random.randint(50_000, 500_000, 20).astype(float),\n",
        "    'Duration_Days': np.random.randint(90, 600, 20),\n",
        "    'Team_Size': np.random.randint(5, 40, 20),\n",
        "    'Manager_Experience_Years': np.random.randint(1, 30, 20),\n",
        "    'Project_Status': np.random.choice(['Completed', 'Ongoing', 'Delayed'], 20),\n",
        "    'Client_Satisfaction': np.random.choice(['Excellent', 'Good', 'Poor'], 20)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.loc[[0, 3], 'Project_Cost_USD'] = [2_000_000, 3_500_000]\n",
        "df.loc[[1, 3], 'Client_Satisfaction'] = ['poor', 'Excelent']\n",
        "df = pd.concat([df, df.loc[[9]]], ignore_index=True)\n",
        "\n",
        "\n",
        "# Introduce Missing Data in 8 Different Ways\n",
        "df.loc[1, 'Project_Cost_USD'] = np.nan       # 1. Listwise deletion example\n",
        "df.loc[2, 'Duration_Days'] = np.nan          # 2. Pairwise deletion example\n",
        "df.loc[3, 'Team_Size'] = np.nan              # 3. Mean/Median/Mode\n",
        "df.loc[4, 'Manager_Experience_Years'] = np.nan # 4. Constant Value\n",
        "df.loc[5, 'Client_Satisfaction'] = np.nan    # 5. Regression\n",
        "df.loc[6, 'Project_Cost_USD'] = np.nan       # 6. KNN\n",
        "df.loc[7, 'Duration_Days'] = np.nan          # 7. Multiple Imputation\n",
        "df.loc[8, 'Project_Cost_USD'] = np.nan       # 8. Expert / Domain estimation\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Missing Data"
      ],
      "metadata": {
        "id": "78wCmUd2A8gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_summary = pd.DataFrame({\n",
        "    'Missing_Count': df.isna().sum(),\n",
        "})\n",
        "print(missing_summary)\n",
        "\n",
        "rows_with_nan = df.isna().any(axis=1).sum()\n",
        "rows_with_nan_percent = df.isna().any(axis=1).mean()*100\n",
        "\n",
        "print(f\"Rows with at least one NaN: {rows_with_nan}, {rows_with_nan_percent}\")"
      ],
      "metadata": {
        "id": "28m_1bg_9Gdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| # | Method              | When to Use                       | Pros                   | Cons                      |\n",
        "| - | ------------------- | --------------------------------- | ---------------------- | ------------------------- |\n",
        "| 1 | Listwise Deletion   | <5% missing, MCAR                 | Easy, clean            | Data loss                 |\n",
        "| 2 | Pairwise Deletion   | Correlation-based analyses        | Keeps more data        | Complex interpretation    |\n",
        "| 3 | Mean/Median/Mode    | MCAR or small gaps                | Simple, quick          | Distorts variance         |\n",
        "| 4 | Constant Value      | Meaningful “Unknown” values       | Keeps structure        | Artificial bias           |\n",
        "| 5 | Regression          | MAR data, linear relation         | Uses info efficiently  | Assumes linearity         |\n",
        "| 6 | KNN                 | Nonlinear, small/moderate dataset | Preserves relations    | Slow on large data        |\n",
        "| 7 | Multiple Imputation | MAR or mixed                      | Statistically rigorous | Computationally heavy     |\n",
        "| 8 | Expert-Based        | MNAR or domain-driven             | Context-aware          | Subjective, not automated |\n"
      ],
      "metadata": {
        "id": "iNmFw5kHNY2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-1) Listwise Deletion"
      ],
      "metadata": {
        "id": "SFZO8lv4BJDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_listwise = df.dropna()\n",
        "print(\"\\n1. Listwise Deletion:\")\n",
        "print(df_listwise)\n"
      ],
      "metadata": {
        "id": "R7NQVjGa7gmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-2) Pairwise Deletion (no specific rows are permanently deleted, A row might be included/excluded in one analysis)"
      ],
      "metadata": {
        "id": "JePNWA5B9obI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "corr_pairwise = numeric_df.corr(method='pearson', min_periods=1)\n",
        "print(\"\\n2. Pairwise Deletion (correlation using available pairs):\")\n",
        "print(corr_pairwise)"
      ],
      "metadata": {
        "id": "gYuZ1fz19oSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-3) Mean/Median/Mode"
      ],
      "metadata": {
        "id": "pRbauDys-VxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "df_mean = df.copy()\n",
        "df_mean['Team_Size'].hist()\n",
        "print(\"skewness:\", df_mean['Team_Size'].skew())\n",
        "df_mean['Team_Size'].fillna(df_mean['Team_Size'].mean(), inplace=True)\n",
        "print(\"\\n3. Mean Imputation (Team_Size):\")\n",
        "print(df[['Project_ID', 'Team_Size']].head(6))\n",
        "print(df_mean[['Project_ID', 'Team_Size']].head(6))\n"
      ],
      "metadata": {
        "id": "1TedFRtm_xCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-4) Constant Value Imputation"
      ],
      "metadata": {
        "id": "2MbQaGkQ_0aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_constant = df.copy()\n",
        "df_constant['Manager_Experience_Years'].fillna(0, inplace=True)\n",
        "print(\"\\n4. Constant Value Imputation (Manager_Experience_Years = 0):\")\n",
        "print(df[['Project_ID', 'Manager_Experience_Years']].head(6))\n",
        "print(df_constant[['Project_ID', 'Manager_Experience_Years']].head(6))\n"
      ],
      "metadata": {
        "id": "M7PrnT4b_0RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-5) Regression"
      ],
      "metadata": {
        "id": "fSCR5YgkABu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg = df.copy()\n",
        "train_data = df_reg.dropna(subset=['Project_Cost_USD', 'Manager_Experience_Years'])\n",
        "reg = LinearRegression()\n",
        "reg.fit(train_data[['Manager_Experience_Years']], train_data['Project_Cost_USD'])\n",
        "\n",
        "missing_idx = df_reg['Project_Cost_USD'].isna()\n",
        "predicted_costs = reg.predict(df_reg.loc[missing_idx, ['Manager_Experience_Years']].fillna(df_reg['Manager_Experience_Years'].mean()))\n",
        "df_reg.loc[missing_idx, 'Project_Cost_USD'] = predicted_costs\n",
        "\n",
        "print(\"\\n5. Regression Imputation (predict Project_Cost_USD using Experience):\")\n",
        "print(df[['Project_ID', 'Project_Cost_USD']].head(8))\n",
        "print(df_reg[['Project_ID', 'Project_Cost_USD']].head(8))\n"
      ],
      "metadata": {
        "id": "CwOxyJpxABlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-6) KNN"
      ],
      "metadata": {
        "id": "fns6nGFBAVn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_knn = df.copy()\n",
        "num_cols = ['Project_Cost_USD', 'Duration_Days', 'Team_Size', 'Manager_Experience_Years']\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "df_knn[num_cols] = imputer.fit_transform(df_knn[num_cols])\n",
        "\n",
        "print(\"\\n6. KNN Imputation (nearest neighbor filling):\")\n",
        "print(df[num_cols].head(8))\n",
        "print(df_knn[num_cols].head(8))\n"
      ],
      "metadata": {
        "id": "ZldOl2gkAVQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-7) Multiple (Iterative) Imputation"
      ],
      "metadata": {
        "id": "3He67pcxAbSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi = df.copy()\n",
        "iter_imputer = IterativeImputer(random_state=0)\n",
        "df_multi[num_cols] = iter_imputer.fit_transform(df_multi[num_cols])\n",
        "\n",
        "print(\"\\n7. Multiple (Iterative) Imputation:\")\n",
        "print(df[num_cols].head(8))\n",
        "print(df_multi[num_cols].head(8))\n"
      ],
      "metadata": {
        "id": "ghE487yDAbHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-8) Expert-Based Estimation"
      ],
      "metadata": {
        "id": "b0ZN4K14RFqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_expert = df.copy()\n",
        "\n",
        "# Estimate Project_Cost = Duration * Team_Size * 300\n",
        "df_expert.loc[df_expert['Project_Cost_USD'].isna(), 'Project_Cost_USD'] = \\\n",
        "    df_expert['Duration_Days'] * df_expert['Team_Size'] * 300\n",
        "\n",
        "print(\"\\n8. Expert-Based Estimation (domain formula applied):\")\n",
        "print(df[['Project_ID', 'Project_Cost_USD']].head(10))\n",
        "print(df_expert[['Project_ID', 'Project_Cost_USD']].head(10))\n"
      ],
      "metadata": {
        "id": "d_EM0h1nNZAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Inconsistencies"
      ],
      "metadata": {
        "id": "32c61tsN1Zzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1) Duplicates"
      ],
      "metadata": {
        "id": "R7KucuDV1kDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "print(\"✅ All Duplicated Rows (including originals):\")\n",
        "print(duplicated_rows)\n",
        "print(f\"\\nTotal duplicated rows found: {duplicated_rows.shape[0]}\")\n",
        "\n",
        "df = df.drop_duplicates(keep='first', ignore_index=True)\n",
        "df\n"
      ],
      "metadata": {
        "id": "FnLlAcP81Ztz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2) Check for Data Type Inconsistencies"
      ],
      "metadata": {
        "id": "K54vVAuZ12CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    if df[col].map(type).nunique() > 1:\n",
        "        print(f\"\\n⚠️ Inconsistent types found in '{col}' column:\")\n",
        "        print(df[col].apply(type).value_counts())\n",
        "        print(df[col].unique())\n",
        "\n",
        "df['Client_Satisfaction'] = (\n",
        "    df['Client_Satisfaction']\n",
        "    .astype(str)\n",
        "    .str.replace(r'[^0-9.]', '', regex=True)\n",
        "    .replace('', np.nan)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "print(df[col].apply(type).value_counts())\n",
        "print(df[col].unique())"
      ],
      "metadata": {
        "id": "fAbU7pQs1143"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-3) Outliers"
      ],
      "metadata": {
        "id": "M_NzNdY_2IMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df['Project_Cost_USD'].quantile(0.25)\n",
        "Q3 = df['Project_Cost_USD'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df_outliers = df[(df['Project_Cost_USD'] < lower_bound) | (df['Project_Cost_USD'] > upper_bound)]\n",
        "print(\"Outliers (Before):\\n\", df_outliers)\n",
        "\n",
        "df_removed = df[(df['Project_Cost_USD'] >= lower_bound) & (df['Project_Cost_USD'] <= upper_bound)]\n",
        "\n",
        "# df_capped = df.copy()\n",
        "# df_capped['Project_Cost_USD'] = np.where(df_capped['Project_Cost_USD'] > upper_bound, upper_bound,\n",
        "#                                          np.where(df_capped['Project_Cost_USD'] < lower_bound, lower_bound,\n",
        "#                                                   df_capped['Project_Cost_USD']))\n",
        "\n",
        "# df_transformed = df.copy()\n",
        "# df_transformed['Project_Cost_USD'] = np.log(df_transformed['Project_Cost_USD'])\n",
        "\n",
        "# median_value = df['Project_Cost_USD'].median()\n",
        "# df_imputed = df.copy()\n",
        "# df_imputed.loc[(df['Project_Cost_USD'] < lower_bound) | (df['Project_Cost_USD'] > upper_bound), 'Project_Cost_USD'] = median_value\n",
        "\n",
        "Q1 = df['Project_Cost_USD'].quantile(0.25)\n",
        "Q3 = df['Project_Cost_USD'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df_outliers = df_removed[(df_removed['Project_Cost_USD'] < lower_bound) | (df_removed['Project_Cost_USD'] > upper_bound)]\n",
        "print(\"Outliers (After):\\n\", df_outliers)"
      ],
      "metadata": {
        "id": "7a3dl0JT2IBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Scaling"
      ],
      "metadata": {
        "id": "nxEFBxo_Ri7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "# Select numerical columns\n",
        "num_cols = ['Project_Cost_USD', 'Duration_Days', 'Team_Size', 'Manager_Experience_Years']\n",
        "\n",
        "# Handle missing values with simple imputation (mean) for demonstration\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n",
        "\n",
        "# Function to visualize before and after scaling\n",
        "def visualize_scaling(original, scaled, method_name):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "    sns.boxplot(data=original, ax=axes[0])\n",
        "    axes[0].set_title('Before Scaling')\n",
        "    sns.boxplot(data=scaled, ax=axes[1])\n",
        "    axes[1].set_title(f'After {method_name} Scaling')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"--- Statistics Before Scaling ---\\n{original.describe()}\\n\")\n",
        "    print(f\"--- Statistics After {method_name} Scaling ---\\n{pd.DataFrame(scaled, columns=original.columns).describe()}\\n\")\n",
        "\n",
        "# 1️⃣ Min-Max Normalization\n",
        "minmax_scaler = MinMaxScaler()\n",
        "scaled_minmax = minmax_scaler.fit_transform(df[num_cols])\n",
        "visualize_scaling(df[num_cols], scaled_minmax, 'Min-Max (Normalization)')\n",
        "\n",
        "# 2️⃣ Standardization (Z-score)\n",
        "standard_scaler = StandardScaler()\n",
        "scaled_standard = standard_scaler.fit_transform(df[num_cols])\n",
        "visualize_scaling(df[num_cols], scaled_standard, 'Standardization (Z-score)')\n",
        "\n",
        "# 3️⃣ Robust Scaling (using median and IQR)\n",
        "robust_scaler = RobustScaler()\n",
        "scaled_robust = robust_scaler.fit_transform(df[num_cols])\n",
        "visualize_scaling(df[num_cols], scaled_robust, 'Robust Scaling')"
      ],
      "metadata": {
        "id": "GOnErDdhRiwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}